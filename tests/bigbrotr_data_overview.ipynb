{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter nbconvert tests/bigbrotr_data_overview.ipynb --to html/pdf --no-input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigbrotr Database Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "import os\n",
    "from bigbrotr import Bigbrotr\n",
    "from event import Event\n",
    "from relay import Relay\n",
    "from relay_metadata import RelayMetadata\n",
    "import utils\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from matplotlib_venn import venn3\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_HOST = 'localhost'\n",
    "DB_PORT =5432\n",
    "DB_USER = 'admin'\n",
    "DB_PASSWORD = 'admin'\n",
    "DB_NAME = 'bigbrotr'\n",
    "\n",
    "bigbrotr = Bigbrotr(\n",
    "    host=DB_HOST,\n",
    "    port=DB_PORT,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    "    dbname=DB_NAME\n",
    ")\n",
    "\n",
    "bigbrotr.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_overview(conn):\n",
    "    q = '''\n",
    "    SELECT table_name, column_name, data_type\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'public'\n",
    "    ORDER BY table_name, ordinal_position;\n",
    "    '''\n",
    "    return pd.read_sql(q, conn)\n",
    "\n",
    "schema_df = get_schema_overview(bigbrotr.conn)\n",
    "for table_name in schema_df['table_name'].unique():\n",
    "    df = schema_df[schema_df['table_name'] == table_name].drop(columns='table_name')\n",
    "    display(table_name.capitalize())\n",
    "    print(df.to_markdown(index=False))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Counts per Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_row_counts(conn):\n",
    "#     q = '''\n",
    "#     SELECT relname as table, n_live_tup as row_count\n",
    "#     FROM pg_stat_user_tables\n",
    "#     ORDER BY n_live_tup DESC;\n",
    "#     '''\n",
    "#     return pd.read_sql(q, conn)\n",
    "\n",
    "# row_counts = get_row_counts(bigbrotr.conn)\n",
    "# display(row_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_gb(b):\n",
    "    return b / (1024 ** 3)\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"bigbrotr\",\n",
    "    user=\"admin\",\n",
    "    password=\"admin\",\n",
    "    host=\"localhost\",\n",
    "    port=5432\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Recupera tutte le tabelle nel tuo schema pubblico\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema = 'public' AND table_type = 'BASE TABLE'\n",
    "\"\"\")\n",
    "tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(\"ðŸ“Š Analisi dello spazio per tabella:\\n\")\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"ðŸ§¾ TABELLA: {table}\")\n",
    "    \n",
    "    # Numero righe\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    total_rows = cursor.fetchone()[0]\n",
    "    print(f\"Numero righe: {total_rows}\")\n",
    "\n",
    "    if total_rows == 0:\n",
    "        print(\" (Tabella vuota)\\n\")\n",
    "        continue\n",
    "\n",
    "    # Calcola dimensione media per colonna\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT string_agg(\n",
    "            'AVG(pg_column_size(\\\"' || column_name || '\\\")) AS \\\"' || column_name || '\\\"',\n",
    "            ', '\n",
    "        )\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_name = %s AND table_schema = 'public'\n",
    "    \"\"\", (table,))\n",
    "    agg_expr = cursor.fetchone()[0]\n",
    "\n",
    "    avg_size_query = f\"SELECT {agg_expr} FROM {table}\"\n",
    "    cursor.execute(avg_size_query)\n",
    "    avg_sizes = cursor.fetchone()\n",
    "    colnames = [desc.name for desc in cursor.description]\n",
    "\n",
    "    print(\"{:<20} {:>12} {:>12}\".format(\"Colonna\", \"Avg (bytes)\", \"Totale (GB)\"))\n",
    "    total_data_bytes = 0\n",
    "\n",
    "    for i, col in enumerate(colnames):\n",
    "        avg_bytes = float(avg_sizes[i])\n",
    "        total_bytes = avg_bytes * total_rows\n",
    "        total_data_bytes += total_bytes\n",
    "        print(\"{:<20} {:>12.2f} {:>12.2f}\".format(col, avg_bytes, bytes_to_gb(total_bytes)))\n",
    "\n",
    "    print(f\"Totale dati stimati: {bytes_to_gb(total_data_bytes):.2f} GB\")\n",
    "\n",
    "    # Recupera indici della tabella con dimensione\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT \n",
    "            indexname, \n",
    "            pg_relation_size(indexrelid) AS size_bytes\n",
    "        FROM pg_indexes\n",
    "        JOIN pg_class ON pg_class.relname = indexname\n",
    "        JOIN pg_index ON pg_class.oid = pg_index.indexrelid\n",
    "        WHERE tablename = %s\n",
    "    \"\"\", (table,))\n",
    "    index_data = cursor.fetchall()\n",
    "\n",
    "    total_index_bytes = 0\n",
    "    print(\"\\nðŸ“¦ Indici:\")\n",
    "    print(\"{:<30} {:>12}\".format(\"Indice\", \"Size (GB)\"))\n",
    "    for index_name, size_bytes in index_data:\n",
    "        total_index_bytes += size_bytes\n",
    "        print(\"{:<30} {:>12.2f}\".format(index_name, bytes_to_gb(size_bytes)))\n",
    "\n",
    "    print(f\"Totale indici: {bytes_to_gb(total_index_bytes):.2f} GB\")\n",
    "    print(f\"Totale complessivo stimato: {bytes_to_gb(total_data_bytes + total_index_bytes):.2f} GB\\n\")\n",
    "    print(\"â”€\" * 60)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relays Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relays_w_metadata = pd.read_sql(\n",
    "    '''\n",
    "    SELECT DISTINCT ON (relay_url) relay_url\n",
    "    FROM relay_metadata\n",
    "    ''', bigbrotr.conn\n",
    ")\n",
    "relays = pd.read_sql('SELECT url, network FROM relays', bigbrotr.conn)\n",
    "relays['metadata'] = relays['url'].isin(relays_w_metadata['relay_url'])\n",
    "with_metadata_count = relays['metadata'].sum()\n",
    "total_relays = relays.shape[0]\n",
    "print(f\"Total relays: {total_relays}, with metadata: {with_metadata_count} ({with_metadata_count/total_relays:.2%})\")\n",
    "print(\"Network distribution for relays with metadata:\")\n",
    "tmp = relays[relays['metadata']]['network'].value_counts().reset_index()\n",
    "tmp['perc'] = tmp['count'] / tmp['count'].sum() * 100\n",
    "for _, row in tmp.iterrows():\n",
    "    print(f\"- {row['network']}: {row['count']} ({row['perc']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relay_metadata = pd.read_sql(\n",
    "    '''\n",
    "    SELECT *\n",
    "    FROM relay_metadata\n",
    "    ''', bigbrotr.conn\n",
    ")\n",
    "relay_metadata = pd.merge(relay_metadata, relays.filter(['url', 'network']).rename(columns={'url': 'relay_url'}), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relay_metadata.value_counts('relay_url').plot(kind='hist', title='Relay metadata count distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = relay_metadata.copy()\n",
    "tmp['date'] = pd.to_datetime(tmp['generated_at'], unit='s').dt.date\n",
    "tmp_daily = tmp.sort_values(['relay_url', 'generated_at']).drop_duplicates(subset=['relay_url', 'date'])\n",
    "tmp_daily = tmp_daily.set_index(['relay_url', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total unique relays with metadata: {tmp_daily.index.get_level_values('relay_url').nunique()}\")\n",
    "\n",
    "# --- Prep: Assume tmp_daily already exists ---\n",
    "# Count number of relays per day\n",
    "daily_counts = tmp_daily.reset_index().groupby('date')['relay_url'].nunique()\n",
    "daily_counts = daily_counts.to_frame(name='relay_count')\n",
    "daily_counts.index = pd.to_datetime(daily_counts.index)\n",
    "\n",
    "# Total number of unique relays (to use as cap)\n",
    "max_relays = tmp_daily.index.get_level_values('relay_url').nunique()\n",
    "\n",
    "# Add week and weekday columns\n",
    "daily_counts['week'] = daily_counts.index.isocalendar().week\n",
    "daily_counts['dow'] = daily_counts.index.weekday  # Monday=0\n",
    "\n",
    "# Pivot for heatmap\n",
    "pivot_table = daily_counts.pivot(index='week', columns='dow', values='relay_count')\n",
    "\n",
    "# Cap values at total number of relays\n",
    "pivot_table = pivot_table.clip(upper=max_relays)\n",
    "\n",
    "# Optional: Replace weekday numbers with labels\n",
    "pivot_table.columns = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "# Plot heatmap with annotations\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pivot_table, cmap='YlGnBu', annot=True, fmt='.0f', linewidths=0.5, linecolor='gray',\n",
    "            vmin=0, vmax=max_relays, cbar_kws={'label': 'Relay Count'})\n",
    "plt.title('Number of Relay URLs Monitored Per Day')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Week Number')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['connection_success', 'nip11_success', 'readable', 'writable', 'openable']\n",
    "networks = ['clearnet', 'tor', 'all']\n",
    "nip11_cols = ['name', 'description', 'banner', 'icon', 'pubkey', 'contact', 'supported_nips', 'software', 'version', 'privacy_policy', 'terms_of_service', 'limitation', 'extra_fields']\n",
    "\n",
    "def mode_of_series(series):\n",
    "    \"\"\"Calculate the mode (most frequent value) of a Series.\"\"\"\n",
    "    if series.empty:\n",
    "        return None\n",
    "    mode_series = series.mode()\n",
    "    if mode_series.empty:\n",
    "        return None\n",
    "    return mode_series.iloc[0]\n",
    "\n",
    "def mode_of_lists(series):\n",
    "    tuples = series.apply(lambda x: tuple(x) if isinstance(x, list) else x)\n",
    "    if tuples.empty:\n",
    "        return []\n",
    "    mode_series = tuples.mode()\n",
    "    if mode_series.empty:\n",
    "        return []\n",
    "    mode_tuple = mode_series.iloc[0]\n",
    "    return list(mode_tuple) if isinstance(mode_tuple, tuple) else mode_tuple\n",
    "\n",
    "def mode_of_dicts(series):\n",
    "    def dict_to_tuple(d):\n",
    "        if isinstance(d, dict):\n",
    "            return tuple(sorted(d.items()))\n",
    "        return d\n",
    "    tuples = series.apply(dict_to_tuple)\n",
    "    if tuples.empty:\n",
    "        return {}\n",
    "    mode_series = tuples.mode()\n",
    "    if mode_series.empty:\n",
    "        return {}\n",
    "    mode_tuple = mode_series.iloc[0]\n",
    "    return dict(mode_tuple) if isinstance(mode_tuple, tuple) else mode_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_rtt_percentile_filter(df, rtt_columns, lower_pct=0.05, upper_pct=0.95):\n",
    "    \"\"\"\n",
    "    df: dataframe originale con 'network' e colonne RTT\n",
    "    rtt_columns: lista colonne RTT su cui filtrare outlier con percentili\n",
    "    lower_pct: percentile inferiore (es. 0.05 = 5%)\n",
    "    upper_pct: percentile superiore (es. 0.95 = 95%)\n",
    "    \"\"\"\n",
    "    df_clean = df.dropna(subset=rtt_columns).copy()\n",
    "    \n",
    "    for col in rtt_columns:\n",
    "        low_val = df_clean[col].quantile(lower_pct)\n",
    "        high_val = df_clean[col].quantile(upper_pct)\n",
    "        df_clean = df_clean[(df_clean[col] >= low_val) & (df_clean[col] <= high_val)]\n",
    "    \n",
    "    mean_rtt = df_clean.groupby('network')[rtt_columns].mean()\n",
    "    best_rtt = mean_rtt.min()\n",
    "    percent_diff = ((mean_rtt - best_rtt) / best_rtt * 100).round(1)\n",
    "    \n",
    "    ax = mean_rtt.plot(kind='bar', figsize=(12, 6))\n",
    "    plt.title(f'Mean RTT by Network (Filtered between {int(lower_pct*100)}% and {int(upper_pct*100)}% percentiles)')\n",
    "    plt.ylabel('RTT (ms)')\n",
    "    plt.xlabel('Network')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Etichette % sopra barre\n",
    "    for i, col in enumerate(mean_rtt.columns):\n",
    "        for j, val in enumerate(mean_rtt[col]):\n",
    "            pct = percent_diff.iloc[j, i]\n",
    "            ax.text(j + i*0.17 - 0.17, val + 1, f'+{pct}%', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Uso:\n",
    "df_tmp = tmp_daily.reset_index()\n",
    "plot_mean_rtt_percentile_filter(df_tmp, ['rtt_open', 'rtt_read', 'rtt_write'], lower_pct=0.05, upper_pct=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_rtt_cdfs_by_network(df, rtt_columns, lower_pct=0.05, upper_pct=0.95):\n",
    "    \"\"\"\n",
    "    df: dataframe con colonna 'network', colonne RTT e multi-indice (relay_id, data_misurazione)\n",
    "    rtt_columns: lista colonne RTT (es. ['rtt_open', 'rtt_read', 'rtt_write'])\n",
    "    lower_pct: percentile inferiore per outlier removal\n",
    "    upper_pct: percentile superiore per outlier removal\n",
    "    \"\"\"\n",
    "    df_clean = df.dropna(subset=rtt_columns).copy()\n",
    "\n",
    "    # Filtro outlier globalmente\n",
    "    for col in rtt_columns:\n",
    "        low = df_clean[col].quantile(lower_pct)\n",
    "        high = df_clean[col].quantile(upper_pct)\n",
    "        df_clean = df_clean[(df_clean[col] >= low) & (df_clean[col] <= high)]\n",
    "\n",
    "    n_networks = len(networks)\n",
    "\n",
    "    fig, axes = plt.subplots(n_networks, 1, figsize=(10, 5 * n_networks), sharex=True)\n",
    "\n",
    "    if n_networks == 1:\n",
    "        axes = [axes]  # Se c'Ã¨ solo un network\n",
    "\n",
    "    for ax, net in zip(axes, networks):\n",
    "        if net == 'all':\n",
    "            df_net = df_clean\n",
    "        else:\n",
    "            df_net = df_clean[df_clean['network'] == net]\n",
    "        for col in rtt_columns:\n",
    "            sorted_vals = np.sort(df_net[col])\n",
    "            cdf = np.linspace(0, 1, len(sorted_vals))\n",
    "            ax.plot(sorted_vals, cdf, label=col)\n",
    "        ax.set_title(f'CDF RTT - Network: {net}')\n",
    "        ax.set_ylabel('CDF')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "\n",
    "    axes[-1].set_xlabel('RTT (ms)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df_tmp = tmp_daily.reset_index()  # Assicurati che 'network' sia colonna, non indice\n",
    "plot_rtt_cdfs_by_network(df_tmp, ['rtt_open', 'rtt_read', 'rtt_write'], lower_pct=0.05, upper_pct=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara i dati heatmap in un dizionario (simula il calcolo precedente)\n",
    "heatmaps_data = {}\n",
    "\n",
    "for network in networks:\n",
    "    if network == 'all':\n",
    "        tmp_network = tmp_daily\n",
    "    else:\n",
    "        tmp_network = tmp_daily[tmp_daily['network'] == network]\n",
    "    mode_df = tmp_network.groupby(level=0)[metrics].agg(mode_of_series)\n",
    "\n",
    "    mode_counts = pd.DataFrame({\n",
    "        col: mode_df[col].value_counts() for col in metrics\n",
    "    }).fillna(0).astype(int)\n",
    "\n",
    "    mode_percent = mode_counts.div(mode_counts.sum(axis=0), axis=1) * 100\n",
    "\n",
    "    heatmaps_data[network] = mode_percent.T\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle(\"Percentage of Relays per Metric\", fontsize=16, y=1.05)\n",
    "\n",
    "for i, network in enumerate(networks):\n",
    "    sns.heatmap(\n",
    "        heatmaps_data[network],\n",
    "        annot=True,\n",
    "        fmt=\".1f\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        cbar=(i == 2),\n",
    "        ax=axs[i],\n",
    "        cbar_kws={'label': 'Percentage (%)'}\n",
    "    )\n",
    "    axs[i].set_title(f\"Network: {network}\", fontsize=12)\n",
    "    axs[i].set_xlabel(\"Value\")\n",
    "    axs[i].set_ylabel(\"Metric\")\n",
    "    axs[i].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_df = tmp_daily.groupby(level=0)[metrics+['network']].agg(mode_of_series)\n",
    "for network in networks:\n",
    "    if network == 'all':\n",
    "        tmp_network = mode_df\n",
    "    else:\n",
    "        tmp_network = mode_df[mode_df['network'] == network]\n",
    "    tmp_network = tmp_network.drop(columns='network')\n",
    "    print(f\"\\nNetwork: {network}\")\n",
    "    display(tmp_network.value_counts().reset_index(name='count').rename(columns={'index': 'value'}).sort_values(by='count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_labels = {\n",
    "    '100': 'Openable only',\n",
    "    '010': 'Readable only',\n",
    "    '001': 'Writable only',\n",
    "    '110': 'Openable & Readable',\n",
    "    '101': 'Openable & Writable',\n",
    "    '011': 'Readable & Writable',\n",
    "    '111': 'Openable & Readable & Writable',\n",
    "}\n",
    "\n",
    "venn_sets = {}\n",
    "\n",
    "# Prepara i set per ogni network\n",
    "for network in networks:\n",
    "    if network == 'all':\n",
    "        tmp_network = tmp_daily\n",
    "    else:\n",
    "        tmp_network = tmp_daily[tmp_daily['network'] == network]\n",
    "    mode_df = tmp_network.groupby(level=0)[['openable', 'readable', 'writable']].agg(mode_of_series)\n",
    "\n",
    "    set_openable = set(mode_df[mode_df['openable'] == True].index)\n",
    "    set_readable = set(mode_df[mode_df['readable'] == True].index)\n",
    "    set_writable = set(mode_df[mode_df['writable'] == True].index)\n",
    "\n",
    "    venn_sets[network] = (set_openable, set_readable, set_writable)\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle(\"Relay Status Combinations\", fontsize=16, y=1.05)\n",
    "\n",
    "for i, network in enumerate(networks):\n",
    "    set_openable, set_readable, set_writable = venn_sets[network]\n",
    "    v = venn3([set_openable, set_readable, set_writable], ax=axs[i])\n",
    "\n",
    "    # Nascondi nomi e numeri sui cerchi\n",
    "    for text in v.set_labels:\n",
    "        if text:\n",
    "            text.set_visible(False)\n",
    "    for subset_id in subset_labels.keys():\n",
    "        label = v.get_label_by_id(subset_id)\n",
    "        if label:\n",
    "            label.set_text('')\n",
    "\n",
    "    total_relays = len(set_openable.union(set_readable).union(set_writable))\n",
    "    sets = [set_openable, set_readable, set_writable]\n",
    "\n",
    "    patches = []\n",
    "    for subset_id, label_text in subset_labels.items():\n",
    "        patch = v.get_patch_by_id(subset_id)\n",
    "        if patch:\n",
    "            included_sets = [subset_id[j] == '1' for j in range(3)]\n",
    "\n",
    "            selected = sets[0] if included_sets[0] else set()\n",
    "            for j in range(1, 3):\n",
    "                if included_sets[j]:\n",
    "                    if selected:\n",
    "                        selected = selected.intersection(sets[j])\n",
    "                    else:\n",
    "                        selected = sets[j]\n",
    "            for j in range(3):\n",
    "                if not included_sets[j]:\n",
    "                    selected = selected.difference(sets[j])\n",
    "\n",
    "            count = len(selected)\n",
    "            percent = (count / total_relays * 100) if total_relays > 0 else 0\n",
    "            label_with_count = f\"{label_text}: {count} ({percent:.1f}%)\"\n",
    "            patches.append(mpatches.Patch(color=patch.get_facecolor(), label=label_with_count))\n",
    "\n",
    "    axs[i].legend(handles=patches, loc='upper right', fontsize=8)\n",
    "    axs[i].set_title(f\"Network: {network}\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results for each relay_url\n",
    "results = []\n",
    "\n",
    "for relay_url, group in tmp_daily.groupby(level='relay_url'):\n",
    "    row = {'relay_url': relay_url}\n",
    "    for col in nip11_cols:\n",
    "        if col not in group.columns:\n",
    "            row[col] = None\n",
    "            continue\n",
    "\n",
    "        if col == 'supported_nips':\n",
    "            row[col] = mode_of_lists(group[col])\n",
    "        elif col in ['limitation', 'extra_fields']:\n",
    "            row[col] = mode_of_dicts(group[col])\n",
    "        else:\n",
    "            row[col] = mode_of_series(group[col])\n",
    "    row['network'] = group['network'].iloc[0]\n",
    "    results.append(row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "nip11_modes_df = pd.DataFrame(results).set_index('relay_url')\n",
    "nip11_modes_df['supported_nips'] = nip11_modes_df['supported_nips'].apply(lambda x: np.nan if x == [] else x)\n",
    "nip11_modes_df['limitation'] = nip11_modes_df['limitation'].apply(lambda x: np.nan if x == {} else x)\n",
    "nip11_modes_df['extra_fields'] = nip11_modes_df['extra_fields'].apply(lambda x: np.nan if x == {} else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara un DataFrame che conterrÃ  tutte le percentuali per network e colonne nip11\n",
    "heatmap_data = pd.DataFrame(index=nip11_cols)\n",
    "\n",
    "for network in networks:\n",
    "    if network == 'all':\n",
    "        tmp_network = nip11_modes_df\n",
    "    else:\n",
    "        tmp_network = nip11_modes_df[nip11_modes_df['network'] == network]\n",
    "\n",
    "    percentages = []\n",
    "    total_relays = tmp_network.shape[0]\n",
    "    for col in nip11_cols:\n",
    "        if col not in tmp_network.columns or total_relays == 0:\n",
    "            perc = 0.0\n",
    "        else:\n",
    "            perc = (tmp_network[col].notna().sum() / total_relays) * 100\n",
    "        percentages.append(perc)\n",
    "    heatmap_data[network] = percentages\n",
    "\n",
    "# Calcola il divario assoluto tra clearnet e tor\n",
    "if 'clearnet' in heatmap_data.columns and 'tor' in heatmap_data.columns:\n",
    "    heatmap_data['divario'] = (heatmap_data['clearnet'] - heatmap_data['tor']).abs()\n",
    "    heatmap_data = heatmap_data.sort_values(by='divario', ascending=False)\n",
    "    heatmap_data = heatmap_data.drop(columns='divario')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=\"YlGnBu\",  # piÃ¹ indicato per divergenze\n",
    "    cbar_kws={'label': 'Percentage (%)'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "\n",
    "plt.title(\"Percentage of Relays per NIP-11 Column and Network (Sorted by Clearnet-Tor Gap)\")\n",
    "plt.ylabel(\"NIP-11 Column\")\n",
    "plt.xlabel(\"Network\")\n",
    "plt.xticks(rotation=0, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_heatmap_data_for_dict_column(df, networks, col_name, sort_by_gap=True):\n",
    "    all_keys = set()\n",
    "    for _, row in df.iterrows():\n",
    "        d = row[col_name]\n",
    "        if isinstance(d, dict):\n",
    "            all_keys.update(d.keys())\n",
    "    all_keys = sorted(all_keys)\n",
    "\n",
    "    heatmap_data = pd.DataFrame(index=all_keys)\n",
    "\n",
    "    for network in networks:\n",
    "        if network == 'all':\n",
    "            tmp_network = df\n",
    "        else:\n",
    "            tmp_network = df[df['network'] == network]\n",
    "\n",
    "        total_relays = tmp_network.shape[0]\n",
    "        percentages = []\n",
    "        for key in all_keys:\n",
    "            if total_relays == 0:\n",
    "                perc = 0.0\n",
    "            else:\n",
    "                count = tmp_network[col_name].apply(lambda d: isinstance(d, dict) and key in d).sum()\n",
    "                perc = (count / total_relays) * 100\n",
    "            percentages.append(perc)\n",
    "        heatmap_data[network] = percentages\n",
    "\n",
    "    if sort_by_gap and 'clearnet' in heatmap_data.columns and 'tor' in heatmap_data.columns:\n",
    "        heatmap_data['gap'] = (heatmap_data['clearnet'] - heatmap_data['tor']).abs()\n",
    "        heatmap_data = heatmap_data.sort_values('gap', ascending=False).drop(columns='gap')\n",
    "    else:\n",
    "        heatmap_data['total'] = heatmap_data.sum(axis=1)\n",
    "        heatmap_data = heatmap_data.sort_values('total', ascending=False).drop(columns='total')\n",
    "\n",
    "    return heatmap_data\n",
    "\n",
    "\n",
    "# Calcola heatmap per limitation ordinata\n",
    "heatmap_limitation = calc_heatmap_data_for_dict_column(nip11_modes_df, networks, 'limitation')\n",
    "\n",
    "# Calcola heatmap per extra_fields ordinata\n",
    "heatmap_extra_fields = calc_heatmap_data_for_dict_column(nip11_modes_df, networks, 'extra_fields')\n",
    "\n",
    "\n",
    "# Plot heatmap limitation\n",
    "plt.figure(figsize=(12, max(6, len(heatmap_limitation)*0.4)))\n",
    "sns.heatmap(\n",
    "    heatmap_limitation,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=\"YlGnBu\",\n",
    "    cbar_kws={'label': 'Percentage (%)'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "plt.title(\"Percentage of Relays per Limitation Type and Network (Ordered by Clearnet-Tor Gap)\")\n",
    "plt.ylabel(\"Limitation Type\")\n",
    "plt.xlabel(\"Network\")\n",
    "plt.xticks(rotation=0, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot heatmap extra_fields\n",
    "plt.figure(figsize=(12, max(6, len(heatmap_extra_fields)*0.4)))\n",
    "sns.heatmap(\n",
    "    heatmap_extra_fields,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=\"YlGnBu\",\n",
    "    cbar_kws={'label': 'Percentage (%)'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "plt.title(\"Percentage of Relays per Extra Field and Network (Ordered by Clearnet-Tor Gap)\")\n",
    "plt.ylabel(\"Extra Field\")\n",
    "plt.xlabel(\"Network\")\n",
    "plt.xticks(rotation=0, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_relays = pd.merge(\n",
    "    tmp_daily.groupby(level=0)[metrics].agg(mode_of_series).reset_index(),\n",
    "    nip11_modes_df.reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara i dati\n",
    "tmp_mode = mode_relays.filter(['relay_url', 'network', 'writable', 'readable', 'limitation']).copy()\n",
    "tmp_mode['restricted_writes'] = tmp_mode['limitation'].apply(lambda x: x.get('restricted_writes', None) if isinstance(x, dict) else False)\n",
    "tmp_mode['payment_required'] = tmp_mode['limitation'].apply(lambda x: x.get('payment_required', None) if isinstance(x, dict) else False)\n",
    "tmp_mode = tmp_mode.drop(columns='limitation')\n",
    "tmp_mode = tmp_mode.set_index(['relay_url', 'network'])\n",
    "\n",
    "# print number of relays\n",
    "print(f\"Number of relays in cleanet and tor: {tmp_mode.shape[0]}\")\n",
    "print(f\"Number of relays in cleanet: {tmp_mode.xs('clearnet', level='network').shape[0]}\")\n",
    "print(f\"Number of relays in tor: {tmp_mode.xs('tor', level='network').shape[0]}\")\n",
    "\n",
    "# Numero di heatmap da plottare\n",
    "n = len(networks)\n",
    "fig, axes = plt.subplots(1, n, figsize=(7 * n, 7))  # figura piÃ¹ larga e alta\n",
    "\n",
    "# Se c'Ã¨ un solo network, axes non Ã¨ una lista â†’ forzalo\n",
    "if n == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, network in zip(axes, networks):\n",
    "    if network == 'all':\n",
    "        group = tmp_mode.copy()\n",
    "    else:\n",
    "        group = tmp_mode.xs(network, level='network')\n",
    "\n",
    "    corr_matrix = group.corr()\n",
    "\n",
    "    # Calcolo della matrice con numero di coppie valide\n",
    "    valid_counts = pd.DataFrame(\n",
    "        np.zeros_like(corr_matrix, dtype=int),\n",
    "        index=corr_matrix.index,\n",
    "        columns=corr_matrix.columns\n",
    "    )\n",
    "\n",
    "    for col1 in group.columns:\n",
    "        for col2 in group.columns:\n",
    "            valid_counts.loc[col1, col2] = group[[col1, col2]].dropna().shape[0]\n",
    "\n",
    "    # Prepara annotazioni combinate: \"r=0.75\\n(n=123)\"\n",
    "    annot_matrix = corr_matrix.copy()\n",
    "    for i in corr_matrix.index:\n",
    "        for j in corr_matrix.columns:\n",
    "            r = corr_matrix.loc[i, j]\n",
    "            n = valid_counts.loc[i, j]\n",
    "            annot_matrix.loc[i, j] = f\"{r:.2f}\\n(n={n})\"\n",
    "\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        annot=annot_matrix,\n",
    "        fmt=\"\",  # fmt vuoto perchÃ© l'annotazione Ã¨ giÃ  formattata\n",
    "        cmap=\"coolwarm\",\n",
    "        center=0,\n",
    "        linewidths=0.7,\n",
    "        linecolor='gray',\n",
    "        annot_kws={\"size\": 11},\n",
    "        cbar_kws={'shrink': 0.8},\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{network}\", fontsize=16)\n",
    "    ax.tick_params(axis='x', rotation=90, labelsize=12)\n",
    "    ax.tick_params(axis='y', rotation=0, labelsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.92])  # spazio per il titolo\n",
    "plt.subplots_adjust(wspace=0.35)  # piÃ¹ spazio fra i plot\n",
    "plt.suptitle(\"Correlation Matrices of Relay Properties by Network\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for network in networks:\n",
    "    if network == 'all':\n",
    "        group = tmp_mode.copy()\n",
    "    else:\n",
    "        group = tmp_mode.xs(network, level='network')\n",
    "\n",
    "    print(f\"\\nNetwork: {network}\")\n",
    "    display(group.value_counts().rename('count').reset_index().sort_values(by='count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'relay_synchronization.csv' not in os.listdir('.'):\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        latest.relay_url AS relay_url,\n",
    "        e.created_at AS timestamp,\n",
    "        latest.seen_at AS seen_at\n",
    "    FROM (\n",
    "        SELECT DISTINCT ON (relay_url)\n",
    "            relay_url,\n",
    "            seen_at,\n",
    "            event_id\n",
    "        FROM\n",
    "            events_relays\n",
    "        ORDER BY\n",
    "            relay_url,\n",
    "            seen_at DESC\n",
    "    ) AS latest\n",
    "    JOIN events e ON e.id = latest.event_id;\n",
    "    \"\"\"\n",
    "    bigbrotr.execute(query)\n",
    "    rows = bigbrotr.fetchall()\n",
    "    df = pd.DataFrame(rows, columns=['relay_url', 'timestamp', 'seen_at'])\n",
    "    df['timestamp_month'] = pd.to_datetime(df['timestamp'], unit='s').dt.to_period('M')\n",
    "    df['seen_at_day'] = pd.to_datetime(df['seen_at'], unit='s').dt.to_period('D')\n",
    "    df = df.sort_values(by=['seen_at_day', 'timestamp_month'], ascending=True)\n",
    "    df.to_csv('relay_synchronization.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relay_synchronization = pd.read_csv('relay_synchronization.csv')\n",
    "# Copia temporanea\n",
    "tmp = relay_synchronization.copy()\n",
    "\n",
    "tmp['timestamp_month_dt'] = tmp['timestamp_month']\n",
    "tmp['seen_at_day_dt'] = tmp['seen_at_day']\n",
    "\n",
    "# Calcola frequenze\n",
    "tmp = (\n",
    "    tmp.groupby(['timestamp_month_dt', 'seen_at_day_dt'])\n",
    "    .size()\n",
    "    .reset_index(name='frequency')\n",
    ")\n",
    "\n",
    "# Pivot\n",
    "heatmap_data = tmp.pivot(index='timestamp_month_dt', columns='seen_at_day_dt', values='frequency')\n",
    "\n",
    "# Ordina lâ€™asse Y dal piÃ¹ recente al meno recente\n",
    "heatmap_data = heatmap_data.sort_index(ascending=False)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    cmap='viridis',\n",
    "    annot=True,\n",
    "    fmt='g',\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray',\n",
    "    cbar_kws={'label': 'Frequency'},\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Format x-axis (daily)\n",
    "ax.set_xlabel(\"Seen At Day\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Format y-axis (monthly)\n",
    "ax.set_ylabel(\"Timestamp Month\")\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "# Titolo\n",
    "ax.set_title(\"Heatmap of Seen At Day vs Timestamp Month (Frequency)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'relay_event_counts.csv' not in os.listdir('.'):\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        relay_url,\n",
    "        COUNT(*) AS event_count\n",
    "    FROM \n",
    "        events_relays\n",
    "    GROUP BY \n",
    "        relay_url\n",
    "    ORDER BY \n",
    "        event_count ASC;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, bigbrotr.conn)\n",
    "    df.to_csv(\"relay_event_counts.csv\", index=False)\n",
    "\n",
    "relay_event_counts = pd.read_csv('relay_event_counts.csv')\n",
    "relay_event_counts = pd.merge(\n",
    "    relay_event_counts,\n",
    "    relay_metadata.filter(['relay_url', 'network']).drop_duplicates(),\n",
    "    on='relay_url'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una colonna, tante righe quanti sono i network\n",
    "fig, axes = plt.subplots(len(networks), 1, figsize=(10, 5 * len(networks)), sharex=False)\n",
    "if len(networks) == 1:\n",
    "    axes = [axes]  # Garantisce che axes sia sempre iterabile\n",
    "\n",
    "# Genera un plot per ogni network\n",
    "for i, network in enumerate(networks):\n",
    "    ax = axes[i]\n",
    "    if network == 'all':\n",
    "        sub_df = relay_event_counts\n",
    "    else:\n",
    "        sub_df = relay_event_counts[relay_event_counts['network'] == network]\n",
    "    \n",
    "    # print top 3 relays with most events\n",
    "    if network != 'all':\n",
    "        top_relays = sub_df.nlargest(5, 'event_count')\n",
    "        print(f\"\\nTop 3 relays with most events in {network}:\")\n",
    "        for _, row in top_relays.iterrows():\n",
    "            print(f\"- {row['relay_url']}: {row['event_count']} events\")\n",
    "        \n",
    "    # Calcola CDF\n",
    "    cdf = sub_df['event_count'].value_counts().sort_index().cumsum() / sub_df['event_count'].count()\n",
    "    \n",
    "    ax.plot(cdf.index, cdf.values, marker='o')\n",
    "    ax.set_title(f'CDF of Relays vs Number of Events â€” Network: {network}')\n",
    "    ax.set_xlabel('Number of Events')\n",
    "    ax.set_ylabel('Cumulative Fraction of Relays')\n",
    "    ax.set_xscale('log')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relay_data = pd.merge(\n",
    "    relay_metadata.filter(['relay_url', 'network']).drop_duplicates(),\n",
    "    relay_synchronization,\n",
    "    on='relay_url'\n",
    ")\n",
    "relay_data = pd.merge(\n",
    "    relay_data,\n",
    "    relay_event_counts.drop(columns='network'),\n",
    "    on='relay_url'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relay_data[relay_data['relay_url'].str.contains('oxt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relay_data[relay_data['relay_url'].str.contains('sov')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
